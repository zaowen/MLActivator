\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2017
%
% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2017}

\usepackage[final]{nips_2017}

% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{nips_2017}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{subfig}         % block figures
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage[pdftex]{graphicx}

\title{A Comparison of Activation Functions in Densely Connected Neural Nets}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
  Zachary Owen
  \thanks{ zaowen.github.io } \\
  Department of Math and Computer Science\\
  South Dakota School of Mines and Technology\\ 
  Rapid City, SD 57701 \\
  \texttt{zacharyowen@acm.} \\
}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\begin{abstract}
        insert abstract here
\end{abstract}

\section{Introduction}

Convolutional neural networks ( CNN's ) are a standard model for identifying structures in visual mediums.
In a world where self driving cars and manufacturing automation are wanting for greater and greater capabilities, CNN's have become deeper and deeper.
The increase in layers is necessary due to the newer and more extensive demands for identifying more varied and complex inputs.
Recent networks containing Convolutions like Highway Networks and Residual Networks have had in excess of 100 layers.
This sort of deep layering causes problems when it comes to backwards and forwards propagation as effects early or late in the network may not properly propagate through or back when it would be necessary.

Earlier this year Huang proposed a new architecture of CNN to simply solve this problem called the Densely Connected Convolutional Network.
This network functions on a stacking of convolutions with the output from every previous convolution as an additional input see Figure \ref{fig:DenseBlock}.

This network is not the focus of this paper however, instead it simply forms a convenient basis for the testing of activators in CNN's.
Since every dense block has identical structure and activators ( ignoring the classification layer ), the activators can be easily swapped out for another activator.
In this paper, we will examine the difference between the different activators supplied by the \textbf{keras python api}.

\subsection{Background}

\subsection{Methods}
The original paper uses the ReLU activator and tests very well on several different common sets including CIFAR10, CIFAR100, SVHN and ILSVRC 2012.
A partial goal of this paper is to show if an activator other than the proposed ReLU can perform better than the one in the original paper.
We will be using both Stochastic Gradient Descent (SGD) and Adam as an optimizer. SGD is used in the original paper, but experimentally, Adam performs slightly quicker on the hardware available.
Additionally fewer dense blocks are used due to lack of memory.

We expect both of these caveats to affect the performance of the overall network.
However we also expect

\begin{figure}
        \centering
        \includegraphics[width=0.3\linewidth]{figures/dense_block}
        \caption{A 5-layer dense block. Each layer takes all preceding convolutions as input.}
        \label{fig:DenseBlock}
\end{figure}

\section{ Initial Testing}

\begin{figure}
        \centering
        \subfloat[ReLU]{
        \includegraphics[width=0.5\linewidth]{figures/Adam30/relu_cifar10.pdf}
        }
        \subfloat[Sigmoid]{
        \includegraphics[width=0.5\linewidth]{figures/Adam30/sigmoid_cifar10.pdf}
        }
        \newline
        \subfloat[SoftMax]{
        \includegraphics[width=0.5\linewidth]{figures/Adam30/softmax_cifar10.pdf}
        }
        \subfloat[Tanh]{
        \includegraphics[width=0.5\linewidth]{figures/Adam30/tanh_cifar10.pdf}
        }
        \caption{Densenet with Diffrent Activators. Adam Optimizer, 30 iterations}
        \label{fig:Adam30}
\end{figure}


\begin{figure}
        \centering
        \includegraphics[width=0.8\linewidth]{figures/SGD3k/relu.pdf}
        \caption{Densenet with ReLUconv. SGD Optimizer, 300 iterations}
\end{figure}

\begin{figure}
        \centering
        \includegraphics[width=0.8\linewidth]{figures/SGD3k/sigmoid.pdf}
        \caption{Densenet with Sigmoidconv. SGD Optimizer, 300 iterations}
\end{figure}

\begin{figure}
        \centering
        \includegraphics[width=0.8\linewidth]{figures/SGD3k/softmax.pdf}
        \caption{Densenet with Softmaxconv. SGD Optimizer, 300 iterations}
\end{figure}

\begin{figure}
        \centering
        \includegraphics[width=0.8\linewidth]{figures/SGD3k/tanh.pdf}
        \caption{Densenet with Tanhconv. SGD Optimizer, 300 iterations}
\end{figure}

\begin{figure}
        \centering
        \includegraphics[width=0.8\linewidth]{figures/Adam500/relu.pdf}
        \caption{Densenet with ReLUconv. Adam Optimizer, 500 iterations}
\end{figure}

\begin{figure}
        \centering
        \includegraphics[width=0.8\linewidth]{figures/Adam500/tanh.pdf}
        \caption{Densenet with Tanhconv. Adam Optimizer, 500 iterations}
\end{figure}

\begin{figure}
        \centering
        \includegraphics[width=0.8\linewidth]{figures/RELU5kcifar10.pdf}
        \caption{Densenet with ReLUconv. Adam Optimizer, 5000 iterations}
\end{figure}



\end{document}
